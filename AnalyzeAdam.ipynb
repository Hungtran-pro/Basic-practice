{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnalyzeAdam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10JGntBvfc1EKmzKYG7EaH2q_RkXCoAhW",
      "authorship_tag": "ABX9TyNNXvFeQUKP1jMdD6JRphDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hungtran-pro/Basic-practice/blob/main/AnalyzeAdam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Import required libraries\n",
        "    \n",
        "    numpy: for scientific computing.\n",
        "    tensorflow: for collecting MNIST data.\n",
        "    math: for mathematic tasks\n",
        "    time: to calculate and estimate the total amount of running time.\n",
        "    os: to access folder's path, file's path\n",
        "    matplotlib.pyplot: to describe and draw histograms.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PJUVwCY31pvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Check wheather the path exists or not\n",
        "'''\n",
        "\n",
        "!ls \"/content/drive/MyDrive/AI projects/Hand_written/mnist.npz\"\n",
        "PATH = \"/content/drive/MyDrive/AI projects/Hand_written/mnist.npz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqvdE2_l1vZE",
        "outputId": "d2dfd71c-50b8-4184-bd71-84009491a398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/AI projects/Hand_written/mnist.npz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Get the trainining dataset and test dataset from tensorflow library.\n",
        "    The dataset includes 70,000 figures for 10 classes (0 - 9), is divided into training dataset and test dataset, with 60k images and 10k images respectively.\n",
        "\n",
        "  Variables:\n",
        "    train_x: contains images in the training dataset.\n",
        "    train_y: contains labels being respective to each image in the training dataset.\n",
        "    test_x: contains images in the test dataset.\n",
        "    test_y: contains labels being respective to each image in the test dataset.\n",
        "'''\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data(PATH)"
      ],
      "metadata": {
        "id": "d_vl8etC2D9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Check the shape of each data and label variables to make sure they're in the right way.\n",
        "'''\n",
        "\n",
        "print(f\"Shape of training X dataset: {train_x.shape}\")\n",
        "print(f\"Shape of training Y dataset: {train_y.shape}\")\n",
        "print(f\"Shape of testing X dataset: {test_x.shape}\")\n",
        "print(f\"Shape of testing Y dataset: {test_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxkBwCRYf-tG",
        "outputId": "faebc636-f67b-49a5-d7f2-f8b882a56f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training X dataset: (60000, 28, 28)\n",
            "Shape of training Y dataset: (60000,)\n",
            "Shape of testing X dataset: (10000, 28, 28)\n",
            "Shape of testing Y dataset: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  To input the data into out neural network, we need to preprocess the data shape.\n",
        "\n",
        "  Expected input shape: (n,m)\n",
        "    - n: a number of features in an image.\n",
        "    - m: a number of images\n",
        "  Expected output shape: (1,m)\n",
        "    - 1: a number represents the label that the image belongs to.\n",
        "    - m: a number of images.\n",
        "'''\n",
        "\n",
        "# Flattern the image data (m, height, weight) -> (m, height x weight)\n",
        "train_x_flatten = train_x.reshape(train_x.shape[0], -1)\n",
        "test_x_flatten = test_x.reshape(test_x.shape[0], -1)\n",
        "\n",
        "# Reshape the label data () -> (1,m)\n",
        "train_y = train_y.reshape(train_y.shape[0], 1)\n",
        "test_y = test_y.reshape(test_y.shape[0], 1)\n",
        "\n",
        "#Normalize input data in range [0,1]\n",
        "train_x = train_x_flatten / 255.0\n",
        "test_x = test_x_flatten / 255.0\n",
        "\n",
        "# Transpose the data matrix to get a right shape input.\n",
        "train_x = np.array(train_x).T\n",
        "train_y = np.array(train_y).T\n",
        "test_x = np.array(test_x).T\n",
        "test_y = np.array(test_y).T"
      ],
      "metadata": {
        "id": "5xIMl4D06h_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Check the image, label data shape again\n",
        "'''\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDwYbSLQ2dZt",
        "outputId": "3899bacf-9a83-4b63-f964-308e53e53183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 60000) (1, 60000)\n",
            "(784, 10000) (1, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "  '''\n",
        "  Intialize weights and bias for the Neural Network\n",
        "  \n",
        "  Input:\n",
        "    layer_dims: contains the dimensions of each layer in neural network\n",
        "    \n",
        "  Variables:\n",
        "\n",
        "    parameter[wi] = (layer_dims[i], layer_dims[i-1])\n",
        "      : contains a \"weight\" matrix of i_th layer\n",
        "    parameter[bi] = (layer_dims[i], layer_dims[i-1])\n",
        "      : contains a \"bias\" vector of u_th layer\n",
        "\n",
        "  Output:\n",
        "    parameters: a python dict contains all parameter[w] and parameter[b]\n",
        "  '''\n",
        "\n",
        "  parameters = {}\n",
        "  for i in range(1, len(layer_dims)):\n",
        "    parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
        "    parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "IvTQD5WB2oLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_adam(parameters) :\n",
        "  '''\n",
        "\n",
        "  Intialize v and s\n",
        "\n",
        "  Input:\n",
        "    parameters: a python dict contains all parameter[w] and parameter[b]\n",
        "\n",
        "  Variables:\n",
        "    \n",
        "    L: the number of layers\n",
        "    v: the refered variables in Momemtum algorithm (with the same dimension of weight matrix and bias vector)\n",
        "    s: the refered variables in RSMP algorithm (with the same dimension of weight matrix and bias vector)\n",
        "  \n",
        "  Output: v, s\n",
        "\n",
        "  '''\n",
        "  L = len(parameters) // 2\n",
        "  v = {}\n",
        "  s = {}\n",
        "\n",
        "  for l in range(L):\n",
        "    v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "    v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "\n",
        "    s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "    s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "  \n",
        "  return v, s"
      ],
      "metadata": {
        "id": "mFL4q8k3kAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 10000, seed = 0):\n",
        "  '''\n",
        "  Creates a list of random minibatches from (X, Y)\n",
        "  \n",
        "  Input:\n",
        "    X -- input data\n",
        "    Y -- true \"label\" vector\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    seed: using for random function in numpy (make a constant random call)\n",
        "\n",
        "  Variables:\n",
        "    permutation: a list of permutation number\n",
        "\n",
        "  Output:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "  '''\n",
        "  \n",
        "  np.random.seed(seed)\n",
        "  m = X.shape[1]\n",
        "  mini_batches = []\n",
        "      \n",
        "  # Shuffle (X, Y)\n",
        "  permutation = list(np.random.permutation(m))\n",
        "  shuffled_X = X[:, permutation]\n",
        "  shuffled_Y = Y[:, permutation]\n",
        "  \n",
        "  inc = mini_batch_size\n",
        "\n",
        "  num_complete_minibatches = math.floor(m / mini_batch_size)\n",
        "  for k in range(0, num_complete_minibatches):\n",
        "    mini_batch_X = shuffled_X[:, k * inc: (k+1) * inc]\n",
        "    mini_batch_Y = shuffled_Y[:, k * inc: (k+1) * inc]\n",
        "    mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "  \n",
        "  # In case the amount of data isn't dividable.\n",
        "  if m % mini_batch_size != 0:\n",
        "    mini_batch_X = shuffled_X[:, m-(m % mini_batch_size) : m]\n",
        "    mini_batch_Y = shuffled_Y[:, m-(m % mini_batch_size) : m]\n",
        "    mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "  \n",
        "  return mini_batches"
      ],
      "metadata": {
        "id": "0VILQkOSARt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  '''\n",
        "  Calculate sigmoid function\n",
        "  '''\n",
        "\n",
        "  A = 1 / (1 + np.exp(-Z))\n",
        "  return A, Z\n",
        "\n",
        "def relu(Z):\n",
        "  '''\n",
        "  Calculate relu function\n",
        "  '''\n",
        "  \n",
        "  A = np.maximum(0, Z)\n",
        "  return A, Z"
      ],
      "metadata": {
        "id": "HZrhEYUCuWfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "  '''\n",
        "  Implement relu backward\n",
        "  '''\n",
        "  \n",
        "  Z = cache\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "  return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "  '''\n",
        "  Implement sigmoid backward\n",
        "  '''\n",
        "\n",
        "  Z = cache\n",
        "  Z_sigmoid = 1 / (1 + np.exp(-Z))\n",
        "  dZ = dA * Z_sigmoid * (1 - Z_sigmoid)\n",
        "  return dZ"
      ],
      "metadata": {
        "id": "nJ1rrsOi5KhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "  '''\n",
        "  Implement linear parts of forward propagation.\n",
        "\n",
        "  Input:\n",
        "    A: activations from the previous layer (size of the previous layer, number of examples)\n",
        "    W: weights of the current layer (size of current layer, size of the previous layer)\n",
        "    b: bias of the current layer (size of current layer, 1)\n",
        "  \n",
        "  Output:\n",
        "    z: pre-activaion parameters (size of current layer, number of examples)\n",
        "    cache: stores \"A\", \"w\", \"b\" for the purpose of calculating back propagation steps.\n",
        "  '''\n",
        "\n",
        "  Z = np.dot(W, A) + b\n",
        "  cache = (A, W, b)\n",
        "\n",
        "  return Z, cache"
      ],
      "metadata": {
        "id": "Bhr5OjNXmqjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  '''\n",
        "  Implement forward propagation in a particular layer\n",
        "\n",
        "  Input:\n",
        "    A_prev: activations of the previous layer (size of the previous layer, number of examples)\n",
        "    w: weights of the current layer (size of current layer, size of the previous layer)\n",
        "    b: bias of the current layer (size of the current layer, 1)\n",
        "    activation: corresponding activation's name for a particular activation of each layer\n",
        "  \n",
        "  Output:\n",
        "    A: activation of the current layer (size of the current layer, number of examples)\n",
        "    cache: stores \"activaion cache\" and \"linear cache\"\n",
        "  '''\n",
        "\n",
        "  if activation == \"relu\":\n",
        "    Z , linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = relu(Z)\n",
        "  elif activation == \"sigmoid\":\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "\n",
        "  cache = (linear_cache, activation_cache)\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "4cyLPakMn9H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "  '''\n",
        "  Implement forward propagation\n",
        "  \n",
        "  Input:\n",
        "    X: input of neural network.\n",
        "    parameters: a dict contains weight, bias parameters\n",
        "\n",
        "  Output:\n",
        "    AL: (Y_hat) the prediction of neural network - list.\n",
        "    caches: contains cache at each specific layers (back propagation calculation purpose)\n",
        "  '''\n",
        "\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2\n",
        "\n",
        "  for l in range(1, L):\n",
        "    A_prev = A\n",
        "    A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
        "    caches.append(cache)\n",
        "\n",
        "  AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
        "  caches.append(cache)\n",
        "  return AL, caches  "
      ],
      "metadata": {
        "id": "8PFM7dsU3syc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "  '''\n",
        "  Compute the cost function\n",
        "  \n",
        "  Input:\n",
        "    AL: the probability corresponding to the label predictions\n",
        "    Y: true label\n",
        "  \n",
        "  Output:\n",
        "    cost: using category entropy function to caculate the cost value\n",
        "  '''\n",
        "\n",
        "  assert(AL.shape == Y.shape)\n",
        "  m = Y.shape[1]\n",
        "  cost = - 1 / m * np.sum(np.sum(np.log(AL) * Y + np.log(1 - AL) * (1-Y), axis = 1), axis = 0)\n",
        "  cost = np.squeeze(cost)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "4THaZyFNpkq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "  '''\n",
        "  Implement the linear portion of back propagation\n",
        "    \n",
        "  Input:\n",
        "    dZ: Gradient of the cost\n",
        "    cache: contains (A_prev, W, b)\n",
        "    A_prev: activation value of the previous layer\n",
        "    W: weights matrix of the current layer\n",
        "    b: bias vector of the current layer\n",
        "  \n",
        "  Output:\n",
        "    dW, db, dA_prev\n",
        "    dA_prev: Gradient of the cost with respect to the activation\n",
        "    dW: Gradient of the cost with respect to the weights\n",
        "    db: Gradient of the cost with respect to the bias\n",
        "  '''\n",
        "\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1] # number of examples\n",
        "\n",
        "  dW = 1 / m * (np.dot(dZ, A_prev.T))\n",
        "  db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "Gin2KhJfwgYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "  '''\n",
        "  Implement the backward propagation\n",
        "  \n",
        "  Input:\n",
        "    dA: Gradient of the cost with respect to the activation\n",
        "    cache: contains (linear_cache, activation_cache)\n",
        "    activation: contains \"relu\" or \"sigmoid\", the activation's name of the layer\n",
        "  \n",
        "  Output:\n",
        "    dA_prev: Gradient of the cost with respect to the activation\n",
        "    dW: Gradient of the cost with respect to the weights\n",
        "    db: Gradient of the cost with respect to the bias\n",
        "  '''\n",
        "  \n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation == 'relu':\n",
        "\n",
        "    dZ = relu_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation == 'sigmoid':\n",
        "    dZ = sigmoid_backward(dA, activation_cache)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "Qcv7lGXV3-Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "  '''\n",
        "  Implement the backward propagation\n",
        "  \n",
        "  Input:\n",
        "    AL: the probability, output of forward propagation\n",
        "    Y: true \"label\"\n",
        "    caches: contains every cache of each layer\n",
        "  \n",
        "  Output:\n",
        "    grads: Gradient Descent for each weight and bias\n",
        "  '''\n",
        "\n",
        "  grads = {}\n",
        "  L = len(caches)\n",
        "  m = AL.shape[1]\n",
        "\n",
        "  #Intialize dAl (gradient descent of the last layer)\n",
        "  dAL = - (np.divide(Y , AL) - np.divide(1 - Y, 1 - AL))\n",
        "  cur_cache = caches[-1]\n",
        "  dA_prev, dW, db = linear_activation_backward(dAL, cur_cache ,'sigmoid')\n",
        "  grads[\"dA\" + str(L-1)] = dA_prev\n",
        "  grads[\"dW\" + str(L)] = dW\n",
        "  grads[\"db\" + str(L)]= db\n",
        "\n",
        "  for l in reversed(range(L-1)):\n",
        "    cur_cache = caches[l]\n",
        "    dA_prev, dW, db = linear_activation_backward(dA_prev, cur_cache, 'relu')\n",
        "    grads[\"dA\" + str(l)] = dA_prev\n",
        "    grads[\"dW\" + str(l+1)] = dW\n",
        "    grads[\"db\" + str(l+1)]= db\n",
        "    \n",
        "  return grads"
      ],
      "metadata": {
        "id": "73o_LrLOyGFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters_with_gd(params, grads, learning_rate):\n",
        "  '''\n",
        "  Update parameters using gradient descent\n",
        "\n",
        "  Input:\n",
        "    params: contains params\n",
        "    grads: contains gradient descent of weight and bias\n",
        "    learning_rate\n",
        "  \n",
        "  Output:\n",
        "    parameters: new updated parameters\n",
        "  '''\n",
        "\n",
        "  parameters = params.copy()\n",
        "  L = len(parameters) // 2\n",
        "  for l in range(L):\n",
        "    parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "    parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "WYXMGM2A1Zgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "  \"\"\"\n",
        "  Update parameters using Adam\n",
        "\n",
        "  Input:\n",
        "    parameters: a python dict containing the parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads: a python dict containing the gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v: Adam variable, moving average of the first gradient, python dict\n",
        "    s: Adam variable, moving average of the squared gradient, python dict\n",
        "    learning_rate\n",
        "    beta1: Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2: Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon: hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "  Output:\n",
        "    parameters --a  python dict containing your updated parameters \n",
        "    v: Adam variable, moving average of the first gradient, python dict\n",
        "    s: Adam variable, moving average of the squared gradient, python dict\n",
        "  \"\"\"\n",
        "  \n",
        "  L = len(parameters) // 2\n",
        "  v_corrected = {}                         \n",
        "  s_corrected = {}\n",
        "  \n",
        "  for l in range(L):\n",
        "    v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "    v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "    v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "    v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "    s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
        "    s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
        "    s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "    s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "    parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
        "    parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
        "\n",
        "  return parameters, v, s"
      ],
      "metadata": {
        "id": "BKuNm1PIuvdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "  '''\n",
        "  Implement Neural Netowrk\n",
        "  '''\n",
        "  np.random.seed(2)\n",
        "  costs = []\n",
        "\n",
        "  # one_hot Y\n",
        "  Y_one_hot = np.zeros((10, Y.shape[1]))\n",
        "  for i in range(Y.shape[1]):\n",
        "    Y_one_hot[Y[0,i],i] = 1\n",
        "  Y = Y_one_hot\n",
        "\n",
        "  parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "  for i in range(0, num_iterations):\n",
        "    AL, caches = L_model_forward(X, parameters)\n",
        "    cost = compute_cost(AL, Y)\n",
        "    grads = L_model_backward(AL, Y, caches)\n",
        "    parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "\n",
        "    if i % 100 == 0 or i == num_iterations - 1:\n",
        "      costs.append(cost)\n",
        "      if print_cost == True:\n",
        "        print(f\"The cost at {i} iteration: {np.squeeze(cost)}\")\n",
        "\n",
        "  return parameters, costs"
      ],
      "metadata": {
        "id": "XQ87h_FX2Vix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_layer_model_with_mini_batches(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=10000, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=3000, print_cost=True):\n",
        "  '''\n",
        "  3-layer neural network model which can be run in different optimizer modes.\n",
        "  \n",
        "  Input:\n",
        "    X: input data, of shape (2, number of examples)\n",
        "    Y: true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    layers_dims: python list, containing the size of each layer\n",
        "    learning_rate\n",
        "    mini_batch_size: the size of a mini batch\n",
        "    beta1: Exponential decay hyperparameter for the past gradients estimates \n",
        "    beta2: Exponential decay hyperparameter for the past squared gradients estimates \n",
        "    epsilon: hyperparameter preventing division by zero in Adam updates\n",
        "    num_epochs: number of epochs\n",
        "    print_cost: True to print the cost every 1000 epochs\n",
        "\n",
        "  Output:\n",
        "    parameters: a python dict containing your updated parameters \n",
        "  '''\n",
        "\n",
        "  np.random.seed(2)\n",
        "  costs = []\n",
        "  t = 0\n",
        "  seed = 2\n",
        "  Y_one_hot = np.zeros((10, Y.shape[1]))\n",
        "  for i in range(Y.shape[1]):\n",
        "    Y_one_hot[Y[0,i],i] = 1\n",
        "  Y = Y_one_hot\n",
        "\n",
        "  parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "  if optimizer == \"gd\":\n",
        "      pass\n",
        "  elif optimizer == \"adam\":\n",
        "      v, s = initialize_adam(parameters)\n",
        "\n",
        "  for i in range(0, num_iterations):\n",
        "    \n",
        "    mini_batches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
        "    average_cost = 0\n",
        "    len_batch = len(mini_batches)\n",
        "    \n",
        "    for mini_batch in mini_batches:\n",
        "      \n",
        "      mini_batch_X, mini_batch_Y = mini_batch\n",
        "      AL, caches = L_model_forward(mini_batch_X, parameters)\n",
        "      cost = compute_cost(AL, mini_batch_Y)\n",
        "      grads = L_model_backward(AL, mini_batch_Y, caches)\n",
        "      \n",
        "      if optimizer == \"gd\":\n",
        "        parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "      elif optimizer == \"adam\":\n",
        "        t = t + 1 # Adam counter\n",
        "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n",
        "      \n",
        "      average_cost += cost\n",
        "\n",
        "    # print cost value\n",
        "    if i % 100 == 0 or i == num_iterations - 1:\n",
        "      costs.append(average_cost/len_batch)\n",
        "      if print_cost == True:\n",
        "        print(f\"The cost at {i} iteration: {np.squeeze(average_cost/len_batch)}\")\n",
        "\n",
        "  return parameters, costs"
      ],
      "metadata": {
        "id": "09Scm9aCDLn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, parameters):\n",
        "  '''\n",
        "    predict output\n",
        "  '''\n",
        "\n",
        "  AL, _ = L_model_forward(test_x, parameters)\n",
        "  return np.squeeze(np.argmax(AL, axis = 0))"
      ],
      "metadata": {
        "id": "WjkJmmbk2uyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Save parameters and cost values for other purpose\n",
        "'''\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "def save_parameter_and_costs(params, costs, path_param, path_cost):\n",
        "  '''\n",
        "    Save weight to the file.\n",
        "  '''\n",
        "\n",
        "  if os.path.exists(path_param):\n",
        "    os.remove(path_param) # Remove the old file before saving the new one\n",
        "  a_file = open(path_param, \"wb\")\n",
        "  pickle.dump(params, a_file)\n",
        "  a_file.close()\n",
        "  if os.path.exists(path_cost):\n",
        "    os.remove(path_cost) # Remove the old file before saving the new one\n",
        "  a_file = open(path_cost, \"wb\")\n",
        "  pickle.dump(costs, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "def get_parameters(path):\n",
        "  '''\n",
        "    Get params from the file path\n",
        "  '''\n",
        "\n",
        "  if os.path.exists(path):\n",
        "    a_file = open(path, \"rb\")\n",
        "    params = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    return params\n",
        "  else:\n",
        "    print(f\"The path {path} doesn't exist!\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "68lO_KRkh4iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_parameters = \"/content/drive/MyDrive/AI projects/Hand_written/parameters.pkl\"\n",
        "path_cost = \"/content/drive/MyDrive/AI projects/Hand_written/cost.pkl\"\n",
        "path_parameters_adam = \"/content/drive/MyDrive/AI projects/Hand_written/parameters_adam.pkl\"\n",
        "path_cost_adam = \"/content/drive/MyDrive/AI projects/Hand_written/cost_adam.pkl\"\n",
        "learning_rate = 0.03\n",
        "layers_dims = [784, 512, 10]"
      ],
      "metadata": {
        "id": "QKlXEi6W59o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time() # Get the starting time\n",
        "\n",
        "# parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2000, print_cost = True)\n",
        "\n",
        "parameters, costs = L_layer_model_with_mini_batches(train_x, train_y, layers_dims, 'adam', num_iterations = 700, print_cost = True)\n",
        "\n",
        "end_time = time.time() # Get the end time\n",
        "\n",
        "total_training_time = end_time - start_time # Calulate an total amount of implementation time\n",
        "\n",
        "print(f\"Total training time with batch gradient descent: {total_training_time}\")"
      ],
      "metadata": {
        "id": "a3I10Ly32xvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f707b6e-d394-4e45-db30-abaee552e911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost at 0 iteration: 6.2060928933856445\n",
            "The cost at 100 iteration: 0.33237219126643874\n",
            "The cost at 200 iteration: 0.17963177618303602\n",
            "The cost at 300 iteration: 0.11261470161647431\n",
            "The cost at 400 iteration: 0.07257406402569923\n",
            "The cost at 500 iteration: 0.04630537013371346\n",
            "The cost at 600 iteration: 0.029074782964666274\n",
            "The cost at 699 iteration: 0.018281070016301682\n",
            "Total training time with batch gradient descent: 4031.313359260559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_parameter_and_costs(parameters, costs, path_parameters_adam, path_cost_adam)\n",
        "print(\"Save params and costs successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhmJkewdn38r",
        "outputId": "8dc4acda-2934-4eb3-8a90-53f25cddc43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save params and costs successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw cost\n",
        "\n",
        "y_hat = predict(test_x, parameters)\n",
        "y_hat = np.reshape(y_hat, 10000)\n",
        "test_y = np.reshape(test_y, 10000)\n",
        "# print(np.mean(y_hat, test_y))\n",
        "print(f\"The accuracy: {np.mean(y_hat == test_y)*100}%\")\n",
        "# plot the cost\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('epochs (per 100)')\n",
        "plt.title(f\"Adam with validation acc: {np.mean(y_hat == test_y)*100}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zjO8fuWS3ywe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "4b3e3c0d-a263-42f0-d74d-da37a54bbafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy: 98.08%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_parameters(path_cost)"
      ],
      "metadata": {
        "id": "2eIgju3YuCbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2v0jB9devQFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}